{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa90e876-1901-4ca3-915a-dca0cefc1823",
   "metadata": {},
   "source": [
    "$$\\textrm{Joaquin Peñuela Parra}$$\n",
    "$$\\textrm{Universidad de los Andes}$$\n",
    "$$\\textrm{Grupo de Física de Altas Energías: Fenomenología de Partículas}$$\n",
    "\n",
    "$\\textbf{Preliminares}$ \n",
    "\n",
    "Las librerías que se usan en este capítulo son las siguientes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28228653-785f-4a5d-937e-0cd0dfe273b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf Uniandes_Framework\n",
    "!git clone https://github.com/Phenomenology-group-uniandes/Uniandes_Framework.git\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(f'{os.getcwd()}/Uniandes_Framework')\n",
    "personal_folder = os.getcwd()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler #Permite balancear datasets\n",
    "import xgboost #Gradient Boosting - Modelo de Machine Learning\n",
    "from sklearn.model_selection import train_test_split #Permite separar los datos en los conjuntos de datos para entrenar y testear el modelo de ML\n",
    "from sklearn.model_selection import GridSearchCV #Permite optimizar la busqueda de parametros de un modelo de ML\n",
    "from sklearn.metrics import accuracy_score #Permite calcular la precisión de los modelos de ML\n",
    "\n",
    "import ml_tools #Herramientas de Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8edef0-acf9-42a7-bcbc-67780c2b9646",
   "metadata": {},
   "source": [
    "En este capítulo se utilizan la lista signals por lo que es necesario volverla a definir para no tener inconvenientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93852b-3f8e-48bf-bae2-aaeb2e04c4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Definamos una lista con las señales y un directorio para guardar las secciones eficaces:\n",
    "signal = \"z\"\n",
    "bkgs = [\"w_jets\", \"ww\", \"wz\", \"zz\", \"ttbar\", \"stop\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f713dc-24cf-47c6-90fe-8d097aa2600c",
   "metadata": {},
   "source": [
    "$\\textbf{Leer los datos de los archivos .csv}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd51e0-53f1-47e0-bf68-f997babac769",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets_correlation = {} #Incluye todas las variables cinemáticas del muon 1 y del muon 2\n",
    "Datasets_Z = {}\n",
    "\n",
    "Datasets_correlation[signal] = pd.read_csv(f'{personal_folder}/CSV_Z_Analisis/Data_correlation/{signal}.csv')\n",
    "del Datasets_correlation[signal][Datasets_correlation[signal].columns[0]]\n",
    "Datasets_Z[signal] = pd.read_csv(f'{personal_folder}/CSV_Z_Analisis/Data_Z/{signal}.csv')\n",
    "del Datasets_Z[signal][Datasets_Z[signal].columns[0]]\n",
    "\n",
    "for bkg in bkgs:\n",
    "    Datasets_correlation[bkg] = pd.read_csv(f'{personal_folder}/CSV_Z_Analisis/Data_correlation/{bkg}.csv')\n",
    "    del Datasets_correlation[bkg][Datasets_correlation[bkg].columns[0]]\n",
    "    Datasets_Z[bkg] = pd.read_csv(f'{personal_folder}/CSV_Z_Analisis/Data_Z/{bkg}.csv')\n",
    "    del Datasets_Z[bkg][Datasets_Z[bkg].columns[0]]\n",
    "\n",
    "cut_flows = pd.DataFrame.to_dict(pd.read_csv(f'{personal_folder}/CSV_Z_Analisis/cut_flows.csv',index_col = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e947c36-8a92-4710-8f74-ae4565fe9b1d",
   "metadata": {},
   "source": [
    "Lo primero que debemos hacer es contatenar las columnas de todos los datasets para la señal y para el background. En general si tenemos dos DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d477c4f-9132-40ed-9e32-6133b3c5037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c904eda-0812-492a-84c9-7f54b8066b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets_Z['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71eb5e6-1a88-4838-8545-eb862d3d15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets_correlation['z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b38122-3833-4cf1-976f-338abdadc43e",
   "metadata": {},
   "source": [
    "En este caso, esas son todas las variables cinemáticas asociadas a la señal Z. Si queremos concatenarlas en solo un DataFrame, para eso hay que usar pd.concat y su parametro axis igualarlo a 1 (cuando vale 0 se concatenan filas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdc0ab-5769-4566-9b79-df5ee2ce82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets['signal'] = pd.concat([Datasets_Z['z'], Datasets_correlation['z']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48d066-01ff-45ba-b479-2b99366ca1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets['signal']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1382d9b-66ef-484c-b55c-7f8f5dd0bb5c",
   "metadata": {},
   "source": [
    "Ahora hagamos lo mismo para todo el background, concatenemos las columnas y luego mezclemoslo todo en un dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb48fc-1abc-427e-8e96-58031c01fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets['BKG'] = pd.DataFrame()\n",
    "for bkg in bkgs:\n",
    "    bkg_con_todas_las_columnas = pd.concat([Datasets_Z[bkg], Datasets_correlation[bkg]], axis = 1) #Se concatenan las columnas\n",
    "    Datasets['BKG'] = pd.concat([Datasets['BKG'], bkg_con_todas_las_columnas], axis = 0) #Se concatenan todos los bkg en uno de muchas filas    \n",
    "    \n",
    "Datasets['BKG'].index = [i for i in range(len(Datasets['BKG']['pT_{Z}(GeV)']))] #Si no se hace esto habrán index repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd5072-dc88-40ea-86fe-de4c9886eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets['BKG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d9e0bf-2624-48b6-9b91-aafa5ccf8bfa",
   "metadata": {},
   "source": [
    "Así, Datasets es un directorio que tiene todas las variables cinematicas de la señal y del background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c4f6e-2ba5-40f6-8db0-9f181e5163b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6ceae-de3f-4d0d-8b01-6f9a44ab17b2",
   "metadata": {},
   "source": [
    "$\\textbf{Preparar todos los datos en un solo Dataset}$\n",
    "\n",
    "Para esto debemos añadir una columna de ceros y unos donde el \"1\" representa que el evento es señal y el \"0\" representa que el evento es background en Datasets['signal'] y Datasets['BKG']. Luego de eso debemos concatenarlos ambos en un nuevo Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211b85e-7730-4763-927b-3adc2cbd50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_signal = {'Label': [1 for evento in Datasets['signal']['pT_{Z}(GeV)']]}\n",
    "Datasets['signal'] = pd.concat([Datasets['signal'], pd.DataFrame.from_dict(labels_signal)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e78325-ba36-4adf-b760-fae0070189e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babdfd0-a9e0-4ccb-ab92-8020b3791f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_bkg = {'Label': [0 for evento in Datasets['BKG']['pT_{Z}(GeV)']]}\n",
    "Datasets['BKG'] = pd.concat([Datasets['BKG'], pd.DataFrame.from_dict(labels_bkg)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02bc29b-9bf4-49da-b71b-4d0b60ba69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets['BKG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b245d85-465b-4883-96b1-13137b9dbaee",
   "metadata": {},
   "source": [
    "Ahora volvamos esos dos datasets uno gigante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31602570-191e-46e1-b051-21634bde00b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data = pd.concat([Datasets['signal'], Datasets['BKG']], axis = 0)\n",
    "Data.index = [i for i in range(len(Data['pT_{Z}(GeV)']))] #Si no se hace esto habrán index repetidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c889aa-20d9-4f64-b21a-38fcd2bb7552",
   "metadata": {},
   "source": [
    "Así,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443bc87-b31b-4964-a945-668103d359fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f85eb-4208-4431-a5b9-950889c7941b",
   "metadata": {},
   "source": [
    "Sin embargo, acá estan ordenados, debemos mezclarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284686c-e8ca-4e9f-9560-96f378dd8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e4c05-32b6-4c1d-b579-cabe1afa8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ada2b-a45f-4b71-b4fb-fb300e9349e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "$\\textbf{Balancear Dataset (CREO QUE ESTE PASO NO ES NECESARIO)}$\n",
    "\n",
    "En este punto es claro que hay más filas de BKG que de signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1560e-dd7a-4cb2-b34a-a2d5714a9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.value_counts('Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648905e-3681-498e-8b15-9479c8b6fedb",
   "metadata": {},
   "source": [
    "Para balancear el dataset es necesario escoger 183580 eventos del BKG de manera aleatoria, esto se puede hacer con RandomUnderSampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad04888-c82e-4983-adaf-80cfa1666bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rus = RandomUnderSampler()\n",
    "# Data, Data['Label'] = rus.fit_resample(Data, Data['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7e564-86a5-47e8-8eb8-be4ebede090a",
   "metadata": {},
   "source": [
    "Así,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0f9f2-03ad-4914-b2f9-57156328cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.value_counts('Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb52b1d-cd69-46b4-9060-1ae988764ddc",
   "metadata": {},
   "source": [
    "$\\textbf{Separar en X y Y}$\n",
    "\n",
    "Debemos separar el Dataset en X y Y. Es decir, en input y output para que más adelante sea posible entrenar el modelo de Machine Learning. En este caso es claro que el output es la columna 'Label' y el input son todas las demás columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699832c2-1be8-4cb1-a6ac-f89c3fc501f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Data.loc[:, Data.columns!= 'Label'] #Todas las filas y sus columnas que sean distintas a 'Label'\n",
    "Y = Data.loc[:, 'Label']  #Todas las filas de la columna 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a5234-9158-44e6-941d-38c7fb5fc753",
   "metadata": {},
   "source": [
    "Así,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afc11d-36b1-4125-aa93-ea996c647c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746e1ad-7728-42cc-99dd-4b3da39ec3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c80a0-d93c-47fa-94bc-f5000b256b81",
   "metadata": {},
   "source": [
    "$\\textbf{Separar conjunto de datos para \"train\" y para \"test\"}$\n",
    "\n",
    "Separemos un 50% del Dataset para entrenar el modelo, un 25% para testearlo y un 25% para validarlo. Para esto podemos hacer uso de la herramienta train_test_split que se importó al inicio de este capítulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42cb24-725d-418a-ba7d-85ca09d85d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.5, random_state = 1) #El random_state es para que siempre que se corra este notebook se haga igual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221dacb-b253-4e33-8fa9-ada3cb803880",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b381d2-425f-4d51-9cc5-735f3290a0d1",
   "metadata": {},
   "source": [
    "Aquí separamos primero en 50-50 con la idea de ahora separar X_train y Y_train otra vez 50-50, es decir un 25-25 del total de datos, esto con el objetivo de poder entrenar y validar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c043a-27eb-45aa-8bf9-10cc46ca3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_test, Y_valid, Y_test = train_test_split(X_test, Y_test, test_size = 0.5, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d97e6a-2845-4051-b9a3-5dd8269eb6c2",
   "metadata": {},
   "source": [
    "Así, nos queda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abff405-e892-4f41-a19c-dcf45199aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape, X_valid.shape # 50 - Entrenar, 25 - Testear y 25 - Validar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647bd60-d52a-4177-bf1e-27b2231fa19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape,Y_test.shape, Y_valid.shape # 50 - Entrenar, 25 - Testear y 25 - Validar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63bc29d-bb3e-4a7b-95af-68ea08b544f5",
   "metadata": {},
   "source": [
    "$\\textbf{Crear modelo de Machine Learning}$\n",
    "\n",
    "Para crear un modelo de Gradient Bosting se puede usar xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935300e4-a375-40ed-82f6-1960c4d75164",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier() #Modelo clasificador de gradient boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4178300c-9722-41c9-b1d8-6d4b65ec6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a03d986-ea52-44e0-91e2-f982347c5e99",
   "metadata": {},
   "source": [
    "Aquí ya podemos usarlo para predecir datos, simplemente hay que usar xgb.predict() donde el parametro sera el input que deseemos evaluar. Por ejemplo, usemos X_valid como input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdf6cb-c4f8-4b44-8ee5-9c94a3a988fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = xgb.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed86567-bf9e-4a26-957a-ff64b5f712f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07500854-e250-4e54-a5e1-53bd74246916",
   "metadata": {},
   "source": [
    "Efectivamente el output es una fila de ceros y unos, comparemos esto con Y_valid que contiene los valores reales de los labels para ver que tan bueno es el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f3316-adfd-4142-afa6-75c6bfb6b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Real': Y_valid, 'Predicción' : Y_predict})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4870fc-963d-459c-ae6e-8e10a1816ea2",
   "metadata": {},
   "source": [
    "Aquí se puede ver que funciona bien en algunos casos y en otros no tanto. Analicemos esto de una forma más rigurosa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75402f6d-882a-4fa7-bfff-5b52203f9a30",
   "metadata": {},
   "source": [
    "$\\textbf{Evaluemos su precisión}$\n",
    "\n",
    "Para esto usemos X_Valid y Y_valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d7d31-7d27-4837-981a-8276e559162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_valid, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f851d6c-01da-4163-831d-6d08e5a29902",
   "metadata": {},
   "source": [
    "Si quisieramos un accuracy más grande es necesario analizar los parametros que contiene el modelo de gradient boosting. Ese modelo tiene un conjunto de parámetros, la idea para mejorar la precisión es buscar los que mejor predigan los datos, para esto lo que se hace es darle opciones al modelo y que el analice todas las posibles combinaciones y así extraíga la mejor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de689e-e654-4cbc-a21a-acad52c5a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'nthreads': [1], \n",
    "             'objective': ['binary:logistic'],\n",
    "             'learning_rate': [0.05,0.1],\n",
    "             'n_estimators': [100,200]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a0af8-59ee-40e1-a9d3-cd02043a8b27",
   "metadata": {},
   "source": [
    "Ahora, usando GridSearcCV se puede entrenar el modelo usando todas las combinaciones posibles de parametros dentro de parameters, esta herramienta buscará la que mejor se ajusta. No obstante, esto se podría demorar bastante, para mejorar su tiempo de ejecución anteriormente dejamos un 25% de los datos para testear, la idea es con ese de 25% definir una función de costo que permita testear cada posible combinación de los parameters, así si en cierta cantidad de rondas la función de costo no mejora entonces se analiza con la siguiente combinación posible de parametros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8d2c2-65de-47b1-8d15-863e642bfe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {'early_stopping_rounds': 10,\n",
    "             'eval_metric': 'logloss',\n",
    "             'eval_set': [(X_test, Y_test)] }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01778fb-9589-49df-9514-45f00e3742bf",
   "metadata": {},
   "source": [
    "Ese diccionario significa que va a analizar hasta la ronda 10 los entrenamientos usando X_test y Y_test evaluados en la función de costo, si en 10 rondas la función de costo no mejora entonces el entrenamiento para, así podemos mejorar el tiempo de ejecución que use GridSearchCV para encontrar los mejores parametros dentro de parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6abb01-44f1-4ede-a38b-8de91257f0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(xgb, parameters, cv=3, scoring='accuracy') #cv=3 es hacer cross validation 3 veces\n",
    "clf.fit(X_train, Y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e795c4f-636c-4387-9b83-6187c2dff1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_xgb = clf.best_estimator_ #Ese es el modelo con la mejor combinación de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f77d54-5751-4051-aaea-c2fdc28f8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae29850-5341-4e4f-b09a-fdf7fd7a7a94",
   "metadata": {},
   "source": [
    "Evaluemos su precisión usando X_Valid y Y_valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a68a16-b34f-47a3-92af-31cf029dc775",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = Best_xgb.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab927c15-0016-447d-8849-9c0cc93aafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_valid, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e11d2ae-5ef4-4788-abe2-a85641cbe96b",
   "metadata": {},
   "source": [
    "Mejoró un poco, así que como se quería es un mejor modelo. Esto se podría mejorar aún más si se pone un mayor número de arboles (n_estimators). La idea es jugar con todos los parametros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b05fd-f0c8-4535-a708-9c3d2f01e00a",
   "metadata": {},
   "source": [
    "$\\textbf{Grafiquemos su distribución de probabilidad (Score)}$\n",
    "\n",
    "Cada modelo de ML tiene una grafica de Score asociada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08255e-46a0-4d21-8669-64853babf816",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dist = Best_xgb.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd0939b-acb8-45b9-bbaf-09c77268d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dist #Contiene para cada valor de X_valid la probabilidad de que su output sea 0 y 1 respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51882a89-393c-4290-8a4a-5b53079be5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Dist[:,0], label = 'P(BKG)', bins = 50, alpha = 0.5)\n",
    "plt.hist(Dist[:,1], label = 'P(Signal)', bins = 50, alpha = 0.5)\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.xlabel('ML-Output')\n",
    "plt.ylabel('N events')\n",
    "plt.title('Score Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f845b-9e3d-4131-afc9-1f73dac3e8aa",
   "metadata": {},
   "source": [
    "Para cada valor de X (ML_Output) se puede calcular una significancia.\n",
    "\n",
    "$$ \\textbf{Significancia} = \\frac{\\textrm{Número de Datos de señal}}{ \\sqrt{\\textrm{Número de Datos de señal + Número de Datos de Background}}} $$\n",
    "\n",
    "Dado que tenemos la probabilidad simplemente habría que multiplicarla por el número total de eventos y así tendríamos el número de eventos de señal y de background respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78f6ac-af2c-4a75-be22-b34d49058509",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_events = len(Dist) #Numero total de eventos\n",
    "\n",
    "Significances = np.array([])\n",
    "\n",
    "for i in range(N_events):\n",
    "    N_Signal = N_events*Dist[1]\n",
    "    N_BKG = N_events*Dist[0]\n",
    "    \n",
    "    Significance = N_Signal / np.sqrt(N_Signal + N_BKG)\n",
    "    \n",
    "    Significances = np.append(Significances, Significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad1d430-9ac5-42a0-a154-f5e214fb35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Significances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52580b2-0d3d-4a6a-8c15-1100946fcf51",
   "metadata": {},
   "source": [
    "Busquemos el máximo para saber cuales son los valores de P(BKG) y P(Signal) que maximizan la significancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d511f7-c68b-4e4d-a45c-383b0c28a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dist[np.argmax(Significances)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255bf43-a477-4783-bf25-43d23a8cb870",
   "metadata": {},
   "source": [
    "Ese sería el mejor corte para calcular la significancia.\n",
    "\n",
    "$\\textbf{Nota:}$ Todo esto se hizó sin recurrir a Pheno_BSM debido a que el código escrito en la carpeta ml_tools está escrito para una estructura de datos particular que no fue la que se uso en estos tutoriales. Sin embargo, si se entiende esto bien no debería haber problema en entender que hace cada función dentro de ml_tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
